## Classification & Segmentation of Income Survey Data

- **Objective 1 â€” Income Classification:** predict whether an individualâ€™s income is **â‰¥ $50K**
- **Objective 2 â€” Segmentation:** cluster the population into **interpretable, actionable personas** for business targeting

Designed to be **reproducible**, **pipeline-driven**, and **production-style**:
- Core logic lives in **`src/`**
- Notebook includes everything from exploration to modelling 

---

## ğŸ“Œ Highlights
- Handles **strong class imbalance (~6% positive)** with PR-AUC focused evaluation
- Uses **survey weights** (`weight`) as `sample_weight` in training + evaluation
- Builds **interpretable personas** via **SVD + KMeans** and validates **stability (ARI)**

---

## ğŸ“ Repository structure

```text
census-income-classification-segmentation/
â”œâ”€â”€ README.md
â”œâ”€â”€ TakeHomeProject
â”‚   â”œâ”€â”€ ML-TakehomeProject.pdf
â”‚   â”œâ”€â”€ census_bereau_columns.csv
â”‚   â”œâ”€â”€ census_bereau_data.csv.zip
â”œâ”€â”€ Report.pdf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ features.py
â”‚   â”œâ”€â”€ train_classifier.py
â”‚   â”œâ”€â”€ eval_classifier.py
â”‚   â”œâ”€â”€ cluster_segments.py
â”‚   â”œâ”€â”€ profile_segments.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ notebook.ipynb
â”œâ”€â”€ figs/
â”‚   â”œâ”€â”€ target_and_age.png
â”‚   â”œâ”€â”€ key_distributions_log.png
â”‚   â”œâ”€â”€ roc_pr_curves.png
â”‚   â”œâ”€â”€ svd_cumvar.png
â”‚   â”œâ”€â”€ cluster_metric_compare.png
â”‚   â”œâ”€â”€ kmeans_elbow.png
â”‚   â”œâ”€â”€ segment_numeric_heatmap.png
â”‚   â””â”€â”€ persona_bubble.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```
---

## âš™ï¸ Environment setup

### 1) Create a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# .venv\Scripts\activate    # Windows
```

2) Install dependencies
```bash
pip install -r requirements.txt
```

## ğŸ“¦ Data
```text
data/raw/
  census_bureau_data.csv
  census_bureau_columns.csv
```
The pipeline expects:
	â€¢	label â†’ used to derive binary target (â‰¥ $50K)
	â€¢	weight â†’ survey sampling weight (used for training/eval + segment weighting; not a predictive feature)
	â€¢	remaining columns â†’ numeric/categorical predictors

## ğŸš€ Quickstart (run end-to-end)

Step 1 â€” Data prep (load + clean + target)

Creates:
	â€¢	data/processed/census_clean.csv

```bash
python -m src.data_prep \
  --raw data/raw/census_bureau_data.csv \
  --columns data/raw/census_bureau_columns.csv \
  --out data/processed/census_clean.csv
```

Step 2 â€” Train classifiers (LR, RF, XGBoost)

Creates:
	â€¢	artifacts/models/best_model.joblib
	â€¢	artifacts/models/model_metrics.csv
	â€¢	artifacts/models/train_meta.json

```bash
python -m src.train_classifier \
  --data data/processed/census_clean.csv \
  --out_dir artifacts/models \
  --seed 42
```

Step 3 â€” Evaluate best classifier (metrics + plots)

Creates:
	â€¢	artifacts/eval/metrics.json
	â€¢	artifacts/eval/metrics.csv
	â€¢	confusion matrices + ROC/PR curves

```bash
python -m src.eval_classifier \
  --data data/processed/census_clean.csv \
  --model_path artifacts/models/best_model.joblib \
  --meta_path artifacts/models/train_meta.json \
  --out_dir artifacts/eval
```

Step 4 â€” Train segmentation model (SVD + KMeans)

Creates:
	â€¢	artifacts/segments/preprocess_clust.joblib
	â€¢	artifacts/segments/svd.joblib
	â€¢	artifacts/segments/kmeans.joblib
	â€¢	artifacts/segments/cluster_assignments.csv
	â€¢	artifacts/segments/cluster_summary.csv
	â€¢	artifacts/segments/metadata.json

```bash
python -m src.cluster_segments \
  --data data/processed/census_clean.csv \
  --out_dir artifacts/segments \
  --k 6 \
  --svd_components 50 \
  --seed 42
```

Step 5 â€” Profile segments (personas + visuals)

Generates segment summaries and persona plots.

```bash
python -m src.profile_segments \
  --data_dir data/processed \
  --segments_dir artifacts/segments \
  --out_dir artifacts/segments_profile
```
Step 5 â€” Profile segments (weighted personas + visuals)

Creates:
	â€¢	artifacts/segments_profile/segment_profile_table.csv
	â€¢	artifacts/segments_profile/persona_map.json
	â€¢	artifacts/segments_profile/segment_top_categories/*.csv
	â€¢	artifacts/segments_profile/figs/segment_numeric_heatmap.png
	â€¢	artifacts/segments_profile/figs/persona_bubble.png

```bash
python -m src.profile_segments \
  --segments_dir artifacts/segments \
  --out_dir artifacts/segments_profile
```

## ğŸ§  Methodology summary

Objective 1 â€” Income Classification

Goal: classify individuals as income â‰¥ $50K.

Key choices:
	â€¢	PR-AUC emphasized due to class imbalance
	â€¢	Survey weights used as sample_weight for:
	â€¢	model training (fit(..., sample_weight=weight))
	â€¢	evaluation metrics (weighted ROC-AUC / PR-AUC / Precision / Recall / F1)
	â€¢	Models trained:
	â€¢	Logistic Regression (scaled numerics)
	â€¢	Random Forest
	â€¢	XGBoost (best tabular baseline)

Artifacts:
	â€¢	best_model.joblib contains the full sklearn Pipeline (preprocess + estimator)
	â€¢	model_metrics.csv compares models using the same threshold

Objective 2 â€” Segmentation (Unsupervised Clustering)

Goal: create interpretable personas to support targeting and messaging strategies.

Pipeline:
	â€¢	Preprocess mixed types:
	â€¢	numeric: median imputation + scaling
	â€¢	categorical: impute "Unknown" + OneHotEncode
	â€¢	Dimensionality reduction:
	â€¢	TruncatedSVD on sparse encoded matrix (PCA analogue)
	â€¢	Clustering:
	â€¢	KMeans (k=6) with n_init=20 and fixed seed
	â€¢	Profiling:
	â€¢	weighted segment size share (weight_share)
	â€¢	weighted income propensity (hi_rate_w)
	â€¢	weighted numeric means (heatmap)
	â€¢	weighted top categories for key categorical variables

Personas are assigned using lightweight heuristics to keep results report-friendly.

ğŸ“Œ Deliverables
	â€¢	Report.pdf â€” final write-up
	â€¢	src/ â€” pipeline scripts (minimal, runnable)
	â€¢	notebook.ipynb â€” EDA + modeling narrative
	â€¢	figs/ â€” figures used in report (optional to regenerate)


## ğŸ” Reproducibility

All scripts accept a --seed argument and use deterministic settings where possible.
Training/evaluation uses the same split parameters via train_meta.json.

