# Classification & Segmentation of Income Survey Data

This repo implements two tasks on the U.S. Census income survey dataset:

- **Objective 1 â€” Income Classification:** predict whether an individualâ€™s income is **â‰¥ $50K**
- **Objective 2 â€” Segmentation:** cluster the population into **interpretable, actionable personas** for business targeting

The workflow is **reproducible**, **pipeline-driven**, and kept **minimal + functional** for the take-home:
- Core logic lives in **`src/`**
- The notebook captures EDA + modelling rationale (feature choices, metrics, plots)

---

## ğŸ“Œ Highlights
- Handles **class imbalance (~6% positive)** with **PR-AUC** as the primary metric
- Uses **survey weights** (`weight`) as `sample_weight` in **training and evaluation**
- Builds personas via **One-Hot Encoding â†’ TruncatedSVD â†’ KMeans**
- Produces **weighted segment profiles** (size share + high-income propensity + top categories + numeric heatmap)

---

## ğŸ“ Repository structure

```text
census-income-classification-segmentation/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed
â”‚   â”œâ”€â”€ raw
â”œâ”€â”€ Report.pdf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ features.py
â”‚   â”œâ”€â”€ train_classifier.py
â”‚   â”œâ”€â”€ eval_classifier.py
â”‚   â”œâ”€â”€ cluster_segments.py
â”‚   â”œâ”€â”€ profile_segments.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ notebook
â”‚   â”œâ”€â”€notebook.ipynb
â”œâ”€â”€ figs/                       
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore

## âš™ï¸ Environment setup

### 1) Create a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# .venv\Scripts\activate    # Windows
```

2) Install dependencies
```bash
pip install -r requirements.txt
```

## ğŸ“¦ Data
```text
Place the raw dataset file under:
data/raw/
```

Example:
```text
data/raw/census.csv
```

The pipeline expects:
	â€¢	label â†’ used to derive binary target (â‰¥ $50K)
	â€¢	weight â†’ survey sampling weight (used for training/evaluation only, not a predictive feature)
	â€¢	remaining columns â†’ numeric/categorical features used by the preprocessing pipeline

Note: data/ is ignored by git (recommended for take-homes). Do not commit raw data.

## ğŸš€ Quickstart (run end-to-end)

Step 1 â€” Data prep (cleaning + feature engineering + split)

Creates:
	â€¢	data/processed/train.csv
	â€¢	data/processed/test.csv
	â€¢	optional metadata files (schema / feature lists)

```bash
python -m src.data_prep \
  --raw_path data/raw/census.csv \
  --out_dir data/processed \
  --test_size 0.2 \
  --seed 42
```

Step 2 â€” Train classifiers (LR, RF, XGBoost)

Trains baseline models and writes the best model artifact.

```bash
python -m src.train_classifier \
  --data_dir data/processed \
  --out_dir artifacts/models \
  --seed 42
```

Step 3 â€” Evaluate best classifier (metrics + plots)

Writes:
	â€¢	metrics table (ROC-AUC, PR-AUC, Precision, Recall, F1)
	â€¢	confusion matrix (raw + weighted if enabled)
	â€¢	ROC and PR curves

```bash
python -m src.eval_classifier \
  --data_dir data/processed \
  --model_path artifacts/models/best_model.joblib \
  --out_dir artifacts/eval
```

Step 4 â€” Train segmentation model (SVD + KMeans)

Creates cluster assignments and stores cluster artifacts.

```bash
python -m src.cluster_segments \
  --data_dir data/processed \
  --out_dir artifacts/segments \
  --k 6 \
  --svd_components 50 \
  --seed 42
```

Step 5 â€” Profile segments (personas + visuals)

Generates segment summaries and persona plots.

```bash
python -m src.profile_segments \
  --data_dir data/processed \
  --segments_dir artifacts/segments \
  --out_dir artifacts/segments_profile
```

## ğŸ§  Methodology summary

Objective 1 â€” Income Classification

Goal: rank and classify individuals as income â‰¥ $50K.

Key design choices:
	â€¢	Strong class imbalance (~6% positive): evaluation emphasizes PR-AUC, not accuracy.
	â€¢	Survey weight is used as sample_weight in training and evaluation to better reflect population-level performance.
	â€¢	Models compared:
	â€¢	Logistic Regression (interpretable baseline)
	â€¢	Random Forest (nonlinear bagging)
	â€¢	XGBoost (boosted trees; strongest tabular baseline)
	â€¢	Hyperparameter-tuned XGBoost (light tuning; kept only if it improves PR-AUC)

Example test-set metrics (threshold=0.5):

| Model | ROC-AUC | PR-AUC | F1 | Precision | Recall |
|---|---:|---:|---:|---:|---:|
| Logistic Regression | 0.9477 | 0.6301 | 0.5192 | 0.7326 | 0.4020 |
| Random Forest | 0.9497 | 0.6610 | 0.4772 | 0.8168 | 0.3370 |
| XGBoost | 0.9565 | 0.6989 | 0.6005 | 0.7587 | 0.4969 |
| Hyperparameter-tuned XGBoost | 0.9527 | 0.6746 | 0.5500 | 0.7743 | 0.4265 |

Threshold note: probability threshold is a deployment knob:
	â€¢	higher threshold â†’ higher precision, lower recall
	â€¢	lower threshold â†’ higher recall, lower precision

Objective 2 â€” Segmentation (Unsupervised Clustering)

Goal: create interpretable personas to support targeting and messaging strategies.

Approach:
	â€¢	Mixed numeric + categorical representation:
	â€¢	numeric: median imputation + scaling
	â€¢	categorical: impute Unknown + one-hot encode
	â€¢	High-dimensional sparse matrix â†’ TruncatedSVD (PCA analogue for sparse data)
	â€¢	Clustering algorithm: KMeans for scalability and interpretability
	â€¢	k selection: elbow + internal metrics (Silhouette / CH / DB) + persona interpretability
	â€¢	Stability: reruns across seeds and checks Adjusted Rand Index (ARI)

Stability example:
	â€¢	ARI mean = 0.936, ARI min = 0.839 (high consistency)

Example personas (k=6):
	â€¢	Affluent Investors
	â€¢	Prime Full-Time Workers
	â€¢	Steady Workers
	â€¢	Low-Income Workers
	â€¢	Older Non-Workers
	â€¢	Dependents

## ğŸ“Œ Deliverables
	â€¢	report/Report.pdf â€” final write-up
	â€¢	figs/ â€” figures used in the report
	â€¢	src/ â€” production-style pipeline scripts
	â€¢	notebooks/ â€” exploration (EDA / modeling / segmentation)

## ğŸ” Reproducibility

All scripts accept a --seed argument and use deterministic settings where possible.
